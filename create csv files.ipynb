{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8485739d-d8da-4155-9e03-18a07f258561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "storage_account_name = \"tarifihicham1cs\"\n",
    "storage_account_access_key = \"Ztcio+a8JNZrV2mX9Gjl73jVSsvU1AVPVztAx0b7eYuZMwwo7u3y6Ko0A1Kjp8/MkgWXuAmN8QS0+ASt722nhA==\"\n",
    "container_name = \"tarifihichamcontainer\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "  storage_account_access_key)\n",
    "\n",
    "month = 1\n",
    "year = 2023\n",
    "\n",
    "while month < 6:\n",
    "    num_days = calendar.monthrange(year, month)[1]\n",
    "    # Generate data for each month\n",
    "    start_date = datetime(year, month, 1)\n",
    "    end_date = datetime(year, month, num_days)\n",
    "    date_generated = [start_date + timedelta(days=x) for x in range(0, (end_date-start_date).days+1)]\n",
    "\n",
    "    transport_types = [\"Bus\", \"Train\", \"Tram\", \"Metro\"]\n",
    "    routes = [\"Route_\" + str(i) for i in range(1, 11)]\n",
    "    stations = [\"Station_\" + str(i) for i in range(1, 21)]\n",
    "\n",
    "    # Randomly select 5 days as extreme weather days\n",
    "    extreme_weather_days = random.sample(date_generated, 5)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for date in date_generated:\n",
    "        for _ in range(32):  # 32 records per day to get a total of 992 records for January\n",
    "            transport = random.choice(transport_types)\n",
    "            route = random.choice(routes)\n",
    "\n",
    "            # Normal operating hours\n",
    "            departure_hour = random.randint(5, 22)\n",
    "            departure_minute = random.randint(0, 59)\n",
    "\n",
    "            # Introducing Unusual Operating Hours for buses\n",
    "            if transport == \"Bus\" and random.random() < 0.05:  # 5% chance\n",
    "                departure_hour = 3\n",
    "\n",
    "            departure_time = f\"{departure_hour:02}:{departure_minute:02}\"\n",
    "\n",
    "            # Normal duration\n",
    "            duration = random.randint(10, 120)\n",
    "\n",
    "            # Introducing Short Turnarounds\n",
    "            if random.random() < 0.05:  # 5% chance\n",
    "                duration = random.randint(1, 5)\n",
    "\n",
    "            # General delay\n",
    "            delay = random.randint(0, 15)\n",
    "\n",
    "            # Weather Impact\n",
    "            if date in extreme_weather_days:\n",
    "                # Increase delay by 10 to 60 minutes\n",
    "                delay += random.randint(10, 60)\n",
    "\n",
    "                # 10% chance to change the route\n",
    "                if random.random() < 0.10:\n",
    "                    route = random.choice(routes)\n",
    "\n",
    "            total_minutes = departure_minute + duration + delay\n",
    "            arrival_hour = departure_hour + total_minutes // 60\n",
    "            arrival_minute = total_minutes % 60\n",
    "            arrival_time = f\"{arrival_hour:02}:{arrival_minute:02}\"\n",
    "\n",
    "            passengers = random.randint(1, 100)\n",
    "            departure_station = random.choice(stations)\n",
    "            arrival_station = random.choice(stations)\n",
    "\n",
    "            data.append([date, transport, route, departure_time, arrival_time, passengers, departure_station, arrival_station, delay])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"Date\", \"TransportType\", \"Route\", \"DepartureTime\", \"ArrivalTime\", \"Passengers\", \"DepartureStation\", \"ArrivalStation\", \"Delay\"])\n",
    "    \n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # Reduce the number of partitions to one\n",
    "    spark_df = spark_df.coalesce(1)\n",
    "\n",
    "    spark_df.write.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/raw/public_transport_data{month}\")\n",
    "    month = month + 1\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create csv files",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
