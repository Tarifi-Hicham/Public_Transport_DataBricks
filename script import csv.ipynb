{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c339e251-71fd-4c81-888c-b631a406d1a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_dataframe(df):\n",
    "    # Extract year\n",
    "    df = df.withColumn(\"Year\", year(df[\"Date\"]))\n",
    "    # Extract month\n",
    "    df = df.withColumn(\"Month\", month(df[\"Date\"]))\n",
    "    # Extract day\n",
    "    df = df.withColumn(\"Day\", dayofmonth(df[\"Date\"]))\n",
    "    # Extract day of week\n",
    "    df = df.withColumn(\"DayOfWeek\", dayofweek(df[\"Date\"]))\n",
    "    # Extract duration (calculate the duration in minutes)\n",
    "    df = df.withColumn(\"Duration\", expr(\"(unix_timestamp(ArrivalTime, 'HH:mm') - unix_timestamp(DepartureTime, 'HH:mm')) / 60\"))\n",
    "    # Calculate average passengers\n",
    "    avgPassengers = df.select(avg(\"Passengers\")).first()[0]\n",
    "    # Extract passengers traffic condition\n",
    "    df = df.withColumn(\"PassengersTraffic\", expr(\"CASE WHEN Passengers <= {0} THEN 'Non' ELSE 'Oui' END\".format(avgPassengers)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calcul_avg(df):\n",
    "    df = df.groupBy(\"Route\").agg(avg(\"Passengers\").alias(\"AvgPassengers\"),avg(\"Delay\").alias(\"AvgDelay\"),count(\"Route\").alias(\"RouteCount\"))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cca86ac-fab6-4a6e-91df-2fe17234d862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,month,dayofmonth,col,dayofweek,to_timestamp,from_unixtime,unix_timestamp,expr,avg,count,when\n",
    "\n",
    "storage_account_name = \"tarifihicham1cs\"\n",
    "storage_account_access_key = \"TYsmJdvNk+gxsVIwuvLq3QYmoG9DIb2vKLbHyw3Cx6nrONcotrdqIPdMbN4nVv4IUrs6NiqRlGa++AStp5lK0w==\"\n",
    "container_name = \"tarifihichamcontainer\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "  storage_account_access_key)\n",
    "\n",
    "original_path = dbutils.fs.ls(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/raw\")\n",
    "\n",
    "for path in original_path:\n",
    "    filepath = dbutils.fs.ls(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/raw/\"+path.name)\n",
    "    for filename in filepath:\n",
    "        if filename.name.endswith(\".csv\"):\n",
    "            df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/raw/{path.name}/{filename.name}\")\n",
    "            \n",
    "            df = transform_dataframe(df)\n",
    "            dfm = calcul_avg(df)\n",
    "            \n",
    "            display(df)\n",
    "            display(dfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec21fe7-d33f-4bed-92d9-54cf320451e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"tarifihicham1cs\"\n",
    "storage_account_access_key = \"TYsmJdvNk+gxsVIwuvLq3QYmoG9DIb2vKLbHyw3Cx6nrONcotrdqIPdMbN4nVv4IUrs6NiqRlGa++AStp5lK0w==\"\n",
    "container_name = \"tarifihichamcontainer\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "  storage_account_access_key)\n",
    "\n",
    "raw = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/raw/\"\n",
    "processed = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/processed/\"\n",
    "\n",
    "raw_files = dbutils.fs.ls(raw)\n",
    "processed_files = dbutils.fs.ls(processed)\n",
    "#raw_csv_files = [f.path for f in raw_files if f.name.endswith(\".csv\")] # Lit of CSV Files\n",
    "#raw_file_count = len(raw_csv_files)\n",
    "for raw in raw_files:\n",
    "    print(raw.name)\n",
    "\n",
    "#print(processed_files)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "script import csv",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
