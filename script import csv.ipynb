{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c339e251-71fd-4c81-888c-b631a406d1a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_dataframe(df):\n",
    "    # Extract year\n",
    "    df = df.withColumn(\"Year\", year(df[\"Date\"]))\n",
    "    # Extract month\n",
    "    df = df.withColumn(\"Month\", month(df[\"Date\"]))\n",
    "    # Extract day\n",
    "    df = df.withColumn(\"Day\", dayofmonth(df[\"Date\"]))\n",
    "    # Extract day of week\n",
    "    df = df.withColumn(\"DayOfWeek\", dayofweek(df[\"Date\"]))\n",
    "    # Extract duration (calculate the duration in minutes)\n",
    "    df = df.withColumn(\"Duration\", expr(\"(unix_timestamp(ArrivalTime, 'HH:mm') - unix_timestamp(DepartureTime, 'HH:mm')) / 60\"))\n",
    "    # Calculate average passengers\n",
    "    avgPassengers = df.select(avg(\"Passengers\")).first()[0]\n",
    "    # Extract passengers traffic condition\n",
    "    df = df.withColumn(\"PassengersTraffic\", expr(\"CASE WHEN Passengers <= {0} THEN 'Non' ELSE 'Oui' END\".format(avgPassengers)))\n",
    "    return df\n",
    "\n",
    "def calcul_avg(df):\n",
    "    df = df.groupBy(\"Route\").agg(avg(\"Passengers\").alias(\"AvgPassengers\"),avg(\"Delay\").alias(\"AvgDelay\"),count(\"Route\").alias(\"RouteCount\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec21fe7-d33f-4bed-92d9-54cf320451e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_file_path(storage_account_name,storage_account_access_key,container_name):\n",
    "\n",
    "    spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_access_key)\n",
    "\n",
    "    raw = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/raw/\"\n",
    "    processed = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/processed/\"\n",
    "    archived = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/archived/\"\n",
    "    \n",
    "    raw_files = dbutils.fs.ls(raw)\n",
    "    processed_files = dbutils.fs.ls(processed)\n",
    "    archived_files = dbutils.fs.ls(archived)\n",
    "\n",
    "    return [raw_files, processed_files, archived_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cca86ac-fab6-4a6e-91df-2fe17234d862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,month,dayofmonth,dayofweek,to_timestamp,from_unixtime,unix_timestamp,expr,avg,count,when,col\n",
    "\n",
    "storage_account_name = \"tarifihicham1cs\"\n",
    "storage_account_access_key = \"OCGL4AOQKWaFu6lezWKGDCVXDe7534tiifLMFUgdrPm6YJ3Vff3CMX5EGbxwIXGgBkdqnO6xomBP+ASti5On2w==\"\n",
    "container_name = \"tarifihichamcontainer\"\n",
    "\n",
    "files_paths = get_file_path(storage_account_name,storage_account_access_key,container_name)\n",
    "files_processed = []\n",
    "for processed in files_paths[1]:\n",
    "    files_processed.append(processed.name)\n",
    "\n",
    "counter = 0\n",
    "for raw in files_paths[0]:\n",
    "    if (raw.name not in files_processed) and (counter < 2):\n",
    "        filepath = dbutils.fs.ls(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/raw/\"+raw.name)\n",
    "        for filename in filepath:\n",
    "            if filename.name.endswith(\".csv\"):\n",
    "                \n",
    "                df = spark.read.format(\"csv\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .load(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/raw/{raw.name}/{filename.name}\")\n",
    "                    \n",
    "                df = transform_dataframe(df)\n",
    "                dfm = calcul_avg(df)\n",
    "                \n",
    "                # Reduce the number of partitions to one\n",
    "                df = df.coalesce(1)\n",
    "                dfm = dfm.coalesce(1)\n",
    "                # Export csv file processed\n",
    "                df.write.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/processed/{raw.name}data\")\n",
    "                # Export csv file of analysing\n",
    "                dfm.write.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/public_transport_data/processed/{raw.name}analyse\")\n",
    "                counter = counter + 1"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "script import csv",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
